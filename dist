import requests
import json
import pandas as pd
import random
from IPython.display import clear_output
clear_output(wait=True)
from datetime import datetime

import itertools
import numpy as np
# import networkx as nx
from igraph import Graph

import concurrent.futures

df=pd.read_csv(r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\input-files\input_with_all_events_summed_19-23.csv')
# d = pd.read_csv('input_with_all_events_summed_minimal.csv')
# df=pd.DataFrame(d)

df['month'] = df['month'].fillna(0).astype(int)
df['year'] = df['year'].fillna(0).astype(int)
df['month_year'] = df['year'].astype(str) + '-' + df['month'].astype(str)
df['month_year']=pd.to_datetime(df['month_year'])
df=df.rename(columns={'orgname':'org'})
#df=df[df['month_year']>='2021-1']
df1=df
df=df[(df['year']==2020) | (df['year']==2023)]
dd=r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\input-files\input_lt-50-4.csv' #load ypur org file names
# dd='input_lt-50-1_minimal.csv'
df3=pd.DataFrame(pd.read_csv(dd))
projects=df3[500:1200]['org']
import time
#d1=pd.read_csv(r'F:\jishnu\Gharchive\Gharchive_data\commits\Complete\apache-commits-20-22.csv')
#ap=pd.DataFrame(d1)
import itertools
#weight_push=1
#weight_release=1
#weight_commit=1
#weight=[weight_push,weight_release,weight_commit]
#weighted_df=df.mul(weights,axis=1)
#df['weight_sum']=weighted_df.sum(axis=1)
#df is the complete commits data from '20 -'22 of all licenses
# ap is the commits of only apaches

def un_weight(u, v, d):
    return 1

def convert_keys_to_strings(data):
    if isinstance(data, dict):
        return {str(key): convert_keys_to_strings(value) for key, value in data.items()}
    elif isinstance(data, list):
        return [convert_keys_to_strings(item) for item in data]
    elif isinstance(data, tuple):
        return str(data)
    else:
        return data
# 6 mounth gap
# gap_month=12
gap_month =12 
#creating the pair distances of all the  bipartite network that will be used for finding the social bonding 

def process_project(proj,df,df1):
    
    focal_proj=[proj]
    df_project=df[df['org']==proj]
    #sort the project leve commits 
    df_project=df_project.sort_values(['org','author_name','month_year'])
    # take the unique devs to get their first commits
    df_project.drop_duplicates(subset='author_name',keep='first',inplace=True)
    # if the month and year of some these first commits are same they are removed , so that we won't be wasting time creating duplicate  networks
    df_project.drop_duplicates(subset=['month','year'],keep='first',inplace=True)
    df_devs=df_project.copy()
    #df_devs['datetime']=pd.to_datetime(df_devs['commit_time'],unit='s')
    proj_dev=df_devs[df_devs['org']==proj]['author_name'].unique()

    df_devs=df_devs.sort_values(['org','month_year','author_name'])

    #iterate over devs inside the proj
    proj_distance={}
    for index,row in df_devs.iterrows():
        developer=row['author_name']
        # month corresponding to first commit
        current_month=row['month']
        # year corresponding to first commit
        current_year=row['year']
         #year_month=current_year+current_month/12
        if current_month-gap_month<=0:
            prev_month=12+current_month-gap_month
            prev_year=current_year-1
        else:
            prev_month=current_month-gap_month
            prev_year=current_year
        # month-year correspong to first commit
        gp1 = (f'{current_year}-{current_month:02d}-01 00:00:00')
        # month-year correspong to gap month prior to first commit
        gp2 = (f'{prev_year}-{prev_month:02d}-01 00:00:00')
        # filter the data between these dates
        filtered_df =df1[(df1['month_year'] < gp1) & (df1['month_year']>= gp2)]
        filtered_df=filtered_df[~filtered_df['org'].isin(focal_proj)]
        # find the commit aggreagted at dev and proj level
        filtered_df= filtered_df.groupby(['author_name', 'org'])['sum_event'].sum().reset_index()

        filtered_projects=filtered_df['org'].unique()
        

        # implement Graph with igraph library
        G = Graph()

        filtered_dev=filtered_df['author_name'].unique()
        
        vertex_mapping = {name: index for index, name in enumerate(set(filtered_df['author_name']).union(filtered_df['org']))}

        G.add_vertices(len(vertex_mapping))

        filtered_df['inverse_commit']=1/filtered_df['sum_event']

        edges = [(vertex_mapping[row['author_name']], vertex_mapping[row['org']]) for index, row in filtered_df.iterrows()]
        weights = filtered_df['inverse_commit'].tolist()



        G.add_edges(edges)
        G.es['weight'] = weights

        filtered_df['inverse_commit']=1/filtered_df['sum_event']
        filtered_df=filtered_df.drop(columns='sum_event',axis=1)


        key = f"{current_month}-{current_year}--{prev_month}-{prev_year}"
        
        proj_dev_tot=df[df['org']==proj]['author_name'].unique()
        dev_no=len(proj_dev_tot)
        input_nodes=set(proj_dev_tot)
        dev_dev_shortest_paths={}

        filtered_nodes_indices = [vertex_mapping[dev_name] for dev_name in proj_dev_tot if dev_name in vertex_mapping]
        unique_pairs = list(itertools.combinations(filtered_nodes_indices, 2))

        for pair in unique_pairs:
            source_index, target_index = pair
            try:
                
                shortest_path = G.distances(source=[source_index], target=[target_index], weights='weight')[0][0]
                shortest_path_uw = G.distances(source=[source_index], target=[target_index], weights=None)[0][0]
            except Exception as e:  
                shortest_path = float('inf')
                shortest_path_uw = float('inf')
 
            dev_name_source = [key for key, value in vertex_mapping.items() if value == source_index][0]
            dev_name_target = [key for key, value in vertex_mapping.items() if value == target_index][0]
            # dev_dev_shortest_paths[(dev_name_source, dev_name_target)] = {'weighted': shortest_path, 'unweighted': shortest_path_uw}
            # swapping the index to output same with original code:
            dev_dev_shortest_paths[(dev_name_target, dev_name_source)] = {'weighted': shortest_path, 'unweighted': shortest_path_uw}

        
        proj_distance[key]=dev_dev_shortest_paths
        
        
    return proj_distance,proj

kk=0
kl=0
executor = concurrent.futures.ThreadPoolExecutor(max_workers=10)
futures = [executor.submit(process_project, proj,df,df1) for proj in projects]
result_dict = {}
results_dict_unweighted = {}
print(datetime.now())
# create  a dict with key and value obtained , which will be used while calculating the bonding


for future in concurrent.futures.as_completed(futures):
    
    shortest_paths,proj = future.result()
    result_dict[proj]=shortest_paths
    #results_dict[proj] = shortest_paths
    rows = []
    
    for org, time_period_data in result_dict.items():
        for time_period, pair_data in time_period_data.items():
            for pair, distance_data in pair_data.items():
                weighted_dist=distance_data['weighted']
                unweighted_dist=distance_data['unweighted']
                rows.append({'proj': org, 'time period': time_period, 'weighted_d':weighted_dist, 'pair': pair, 'unweighted_d': unweighted_dist})

    data_to_save = pd.DataFrame(rows)
    
    
    if kk==0:
        data_to_save.to_csv(r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\network distance\chunked_files\2k1923\distance_new19231_prachi.csv',index=False)
        kk=1
    else:
        data_to_save.to_csv(r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\network distance\chunked_files\\2k1923\distance_new19231_prachi.csv',mode='a',header=False,index=False)
        
    data={'proj':[proj],'time':[datetime.now()]}
    d=pd.DataFrame(data)
    if kl==0:
        d.to_csv(r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\network distance\chunked_files\\2k1923\progress_new19231_prachi.csv',index=False)
        kl=1
    else:
        d.to_csv(r'D:\jishnu\OneDrive - Indian School of Business\Gharchive\network distance\chunked_files\\2k1923\progress_new19231_prachi.csv',mode='a',header=False,index=False)
    result_dict={}
    print(datetime.now(),proj, flush=True)
    #results_dict_unweighted[key]=unweighted_paths
    
executor.shutdown()



